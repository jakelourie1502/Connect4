{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from IPython.display import clear_output\n",
    "from tabulate import tabulate\n",
    "import copy\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROW_COUNT = 6\n",
    "COLUMN_COUNT = 7\n",
    " \n",
    "def create_board():\n",
    "    board = np.zeros((6,7))\n",
    "    return board\n",
    " \n",
    "def drop_piece(board,row,col,piece):\n",
    "    board[row][col]= piece\n",
    "\n",
    "def get_valid_locations(board, ROW_COUNT,COLUMN_COUNT):\n",
    "    top_row = board[ROW_COUNT-1]\n",
    "    valid_acts = []\n",
    "    for i in range(COLUMN_COUNT):\n",
    "        if top_row[i] == 0:\n",
    "            valid_acts.append(i)\n",
    "    return valid_acts\n",
    "\n",
    "def is_valid_location(board,col):\n",
    "    return  col < COLUMN_COUNT and board[5][col]==0\n",
    " \n",
    "def get_next_open_row(board,col):\n",
    "    for r in range(ROW_COUNT):\n",
    "        if board[r][col]==0:\n",
    "            return r\n",
    "    \n",
    "def winning_move(board, piece):\n",
    "    # Check horizontal locations for win\n",
    "    for c in range(COLUMN_COUNT-3):\n",
    "        for r in range(ROW_COUNT):\n",
    "            if board[r][c] == piece and board[r][c+1] == piece and board[r][c+2] == piece and board[r][c+3] == piece:\n",
    "                return True\n",
    " \n",
    "    # Check vertical locations for win\n",
    "    for c in range(COLUMN_COUNT):\n",
    "        for r in range(ROW_COUNT-3):\n",
    "            if board[r][c] == piece and board[r+1][c] == piece and board[r+2][c] == piece and board[r+3][c] == piece:\n",
    "                return True\n",
    " \n",
    "    # Check positively sloped diaganols\n",
    "    for c in range(COLUMN_COUNT-3):\n",
    "        for r in range(ROW_COUNT-3):\n",
    "            if board[r][c] == piece and board[r+1][c+1] == piece and board[r+2][c+2] == piece and board[r+3][c+3] == piece:\n",
    "                return True\n",
    " \n",
    "    # Check negatively sloped diaganols\n",
    "    for c in range(COLUMN_COUNT-3):\n",
    "        for r in range(3, ROW_COUNT):\n",
    "            if board[r][c] == piece and board[r-1][c+1] == piece and board[r-2][c+2] == piece and board[r-3][c+3] == piece:\n",
    "                return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def turn(player,board,agent='human',valid=True):\n",
    "    \n",
    "    if agent == 'human':\n",
    "        if player == -1: name = 2\n",
    "        else: name = 1\n",
    "        \n",
    "        if valid: \n",
    "            text = 'Make your Selection(0-6):'\n",
    "        else: \n",
    "            text = 'Invalid choice, make new selection(0-6):'\n",
    "\n",
    "        col = int(input(f\"Player {name}, {text}\"))\n",
    "    \n",
    "    else:\n",
    "        valid_acts = get_valid_locations(board,ROW_COUNT=ROW_COUNT,COLUMN_COUNT = COLUMN_COUNT)\n",
    "        col = agent.make_choice(board,valid_acts,player)\n",
    "\n",
    "    if is_valid_location(board,col):\n",
    "        row = get_next_open_row(board,col)\n",
    "        drop_piece(board,row,col,player)    \n",
    "        return col\n",
    "    else: \n",
    "        turn(player,board,agent=agent, valid=False)\n",
    "\n",
    "            \n",
    "def play_game(init_board = np.zeros((ROW_COUNT,COLUMN_COUNT)), agent_1 = 'human',agent_2 = 'human',printy=True,starting_player=1):\n",
    "    board = init_board\n",
    "    \n",
    "    if printy: \n",
    "        print(tabulate(np.flip(board,0)))\n",
    "    game_over = False\n",
    "    player = starting_player\n",
    "    agent = agent_1\n",
    "    \n",
    "    while not game_over:\n",
    "        if player == 1:\n",
    "            agent = agent_1\n",
    "        else:\n",
    "            agent = agent_2\n",
    "        \n",
    "        t = turn(player,board, agent)\n",
    "        \n",
    "        if printy: \n",
    "            clear_output()\n",
    "            printable_board = np.where(board==-1,2,board)\n",
    "            print(tabulate(np.flip(printable_board,0)))\n",
    "            print(f'Move played: {t}')\n",
    "\n",
    "        if winning_move(board, player): \n",
    "            if printy: \n",
    "                print(f'player {player} won')\n",
    "                time.sleep(5)\n",
    "            game_over = True\n",
    "            return player\n",
    "        \n",
    "        if board.all() != 0:\n",
    "            game_over = True\n",
    "            return 0\n",
    "        player*=-1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# play_game()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class random_agent():\n",
    "    def __init__(self,low,high,agent_name=\"random ronald\"):\n",
    "        self.name = agent_name\n",
    "        self.low = low\n",
    "        self.high = high\n",
    "    def make_choice(self,board,valid_acts,player):\n",
    "        try: return np.random.choice(valid_acts)\n",
    "        except: print('hi',board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_ronald = random_agent(0,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-  -  -  -  -  -  -\n",
      "0  0  0  0  0  0  0\n",
      "0  0  0  0  0  0  0\n",
      "0  0  0  0  0  0  0\n",
      "0  0  0  0  0  0  0\n",
      "0  0  0  0  0  0  0\n",
      "0  0  0  0  0  0  0\n",
      "-  -  -  -  -  -  -\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-50e2255502dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mboard\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_board\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplay_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0magent_1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0magent_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_ronald\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-6683ba37c693>\u001b[0m in \u001b[0;36mplay_game\u001b[0;34m(init_board, agent_1, agent_2, printy, starting_player)\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent_2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplayer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprinty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-6683ba37c693>\u001b[0m in \u001b[0;36mturn\u001b[0;34m(player, board, agent, valid)\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Invalid choice, make new selection(0-6):'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Player {name}, {text}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m             )\n\u001b[0;32m--> 860\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    861\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "board=create_board()\n",
    "play_game(board,agent_1='human',agent_2 = random_ronald)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    def __init__(self,board, action, player, parent = None,actions=[0,1,2,3,4,5,6], done=False,cupt = 5, gamma = 0.9):\n",
    "        \n",
    "        self.board = copy.deepcopy(board)\n",
    "        self.parent = parent\n",
    "        self.action = action\n",
    "        self.children = []\n",
    "        self.n = 0\n",
    "        self.player = player\n",
    "        self.q_value = 0\n",
    "        self.actions = actions\n",
    "        self.done = done\n",
    "        self.cupt = cupt\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def is_root(self):\n",
    "        return self.parent is None\n",
    "    \n",
    "    def select_best_leaf(self):\n",
    "\n",
    "        if self.n == 0 or self.done:\n",
    "            return self\n",
    "        if len(self.children)==0:\n",
    "            self.expand()\n",
    "        best_child = self.children[np.argmax([x.ucb_value() for x in self.children])]\n",
    "        return best_child.select_best_leaf()\n",
    "         \n",
    "        \n",
    "    def ucb_value(self):\n",
    "        #q values are for the player in that state, so the neg of the q value is the player taking the action that gets you there.\n",
    "        return -self.q_value + (self.cupt/np.sqrt(2))*np.sqrt(2*np.log(self.parent.n))/(self.n+1e-5)\n",
    "        \n",
    "    def expand(self):\n",
    "        for i in self.actions:\n",
    "            board_copy = copy.deepcopy(self.board)\n",
    "            if is_valid_location(board_copy,i):\n",
    "                row = get_next_open_row(board_copy,i)\n",
    "                drop_piece(board_copy,row,i,self.player) \n",
    "            done = winning_move(board_copy,self.player)\n",
    "            child = Node(board=board_copy,action=i,player=self.player*-1,parent=self,done=done,cupt=self.cupt, gamma=self.gamma)\n",
    "            self.children.append(child)\n",
    "\n",
    "        #should expand and then end with 'select best_leaf'\n",
    "    \n",
    "    def propagate(self,winner):\n",
    "        self.q_value = (self.q_value * self.n ) + winner*self.player\n",
    "        self.n+=1\n",
    "        self.q_value /= self.n\n",
    "        if not self.is_root():\n",
    "            self.parent.propagate(winner)\n",
    "\n",
    "            \n",
    "def simulation(board,player,iterations, actions=[0,1,2,3,4,5,6], cupt=1, gamma = 0.9):\n",
    "    reward_log =[]\n",
    "    starting_point = board\n",
    "    game_over = False\n",
    "    root = Node(board, action=None,player=player,parent=None,actions=actions, cupt=cupt, gamma = gamma)\n",
    "    for i in range(iterations):\n",
    "        #select the ubc recursion\n",
    "        node = root\n",
    "        node = node.select_best_leaf()\n",
    "        boardy = copy.deepcopy(node.board)\n",
    "        \n",
    "        #check if that board is complete\n",
    "        if node.done: \n",
    "            reward = -node.player\n",
    "\n",
    "        elif boardy.all() != 0:\n",
    "            reward = 0\n",
    "        #play out game randomly\n",
    "        else:\n",
    "            reward = play_game(boardy,agent_1 = random_ronald, agent_2 = random_ronald, printy=False, starting_player=node.player) \n",
    "        node.propagate(reward)\n",
    "    return root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time_start = time.time()\n",
    "# board= create_board()\n",
    "# node = simulation(board, 1, 400)\n",
    "# print('This took' ,round(-time_start + time.time(),2),' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class monte_carlo_player:\n",
    "    def __init__(self,simulations, actions=7,gamma=0.9, cupt =5):\n",
    "        self.simulations = simulations\n",
    "        self.actions = actions\n",
    "        self.gamma = gamma\n",
    "        self.cupt = cupt\n",
    "    def make_choice(self,board,actions_, player):\n",
    "        available_options = []\n",
    "        root = simulation(board, player, self.simulations, actions=actions_, cupt = self.cupt, gamma = self.gamma)\n",
    "        action = actions_[np.argmin([x.q_value for x in root.children])]\n",
    "        return action\n",
    "\n",
    "magic_monty1 = monte_carlo_player(150,cupt=1)\n",
    "magic_monty2 = monte_carlo_player(5000,cupt=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'alfie' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-1f4939e710b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mboard\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_board\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mwinner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplay_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_board\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0magent_1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malfie\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0magent_2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmagic_monty1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprinty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mwinners\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwinner\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# board=create_board()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'alfie' is not defined"
     ]
    }
   ],
   "source": [
    "winners = []\n",
    "for i in range(100):\n",
    "    board=create_board()\n",
    "    winner = play_game(init_board=board,agent_1=alfie ,agent_2=magic_monty1,printy=False)\n",
    "    winners.append(winner)\n",
    "    # board=create_board()\n",
    "    # winner = play_game(init_board=board,agent_1='human',agent_2=magic_monty1,printy=False)\n",
    "    # winners.append(winner*-1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---\n",
    "\n",
    "#### ALPHA-ZERO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02214285714285714"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([0.15,0.15,0.17,0.2,0.23,0.05,0.05])\n",
    "y = np.array([0.2, 0.1, 0.1,0.3,0.1, 0.15, 0.05])\n",
    "x@y/7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Linear, ReLU, Softmax, Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LambdaLayer(torch.nn.Module):\n",
    "    def __init__(self, lambd):\n",
    "        super(LambdaLayer, self).__init__()\n",
    "        self.lambd = lambd\n",
    "    def forward(self, x):\n",
    "        return self.lambd(x)\n",
    "\n",
    "def lambd(x):\n",
    "    if x < 0:\n",
    "        return -1\n",
    "    else:\n",
    "        return 1\n",
    "        \n",
    "class Linear_Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #general use\n",
    "        self.relu = ReLU()\n",
    "        self.dense1 = Linear(42,50)\n",
    "        self.dense2 = Linear(50,50)\n",
    "        self.dense3 = Linear(50,30)\n",
    "        self.dense4p = Linear(30,7)\n",
    "        self.dense4r = Linear(30,1)\n",
    "        self.softmax = Softmax(dim=0)\n",
    "        self.sigmoid = Sigmoid()\n",
    "        self.lamb = LambdaLayer(lambd)\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "        \n",
    "    def forward(self,flat_board): \n",
    "        x1 = self.dense1(flat_board); x1=self.relu(x1)\n",
    "        x2 = self.dense2(x1); x2=self.relu(x2)\n",
    "        x3 = self.dense3(x2); x3=self.relu(x3)\n",
    "        x4p = self.dense4p(x3)\n",
    "        x4r = self.dense4r(x3)\n",
    "        probs = self.softmax(x4p)\n",
    "        v = self.tanh(x4r)\n",
    "#         v = self.lamb(v)\n",
    "        return probs, v\n",
    "    \n",
    "class Dataset(torch.utils.data.Dataset):      \n",
    "    def __init__(self, x, y_P, y_R): \n",
    "        self.x = x.float()\n",
    "        self.y_P = y_P.float()\n",
    "        self.y_R = y_R.float()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index],self.y_P[index],self.y_R[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y_R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AZ_node():\n",
    "    def __init__(self, board, action, player, model, actions=7, parent=None, done=False, cupt=1, gamma =0.9, valid=True):\n",
    "        self.n = 0\n",
    "        self.board = board\n",
    "        self.player = player\n",
    "        self.parent = parent\n",
    "        self.children = []\n",
    "        self.action = action\n",
    "        self.model = model\n",
    "        self.prob_vector, self.exp_value = self.model((torch.from_numpy(board.flatten()*self.player)).float())\n",
    "        self.prob_vector = self.prob_vector.detach().numpy()\n",
    "        self.q_value = 0\n",
    "        self.cupt = cupt\n",
    "        self.actions = 7\n",
    "        self.gamma = gamma\n",
    "        self.done = done\n",
    "        self.valid = valid\n",
    "\n",
    "        \n",
    "    def perform_mcts_search(self):\n",
    "        if self.n == 0:\n",
    "            return self,self.exp_value * self.player #multiply by self.player so it puts in right format\n",
    "\n",
    "        if self.done:\n",
    "            return self,-self.player\n",
    "        \n",
    "\n",
    "        if len(self.children)==0:\n",
    "            self.expand()\n",
    "        best_child = self.children[np.argmax([x.ucb_value() for x in self.children])]\n",
    "        return best_child.perform_mcts_search()\n",
    "\n",
    "    def ucb_value(self):\n",
    "        if not self.valid: \n",
    "            return float('-inf')\n",
    "        return  ( -self.q_value + ( (self.cupt * self.parent.prob_vector[self.action] * np.sqrt(self.parent.n)) / (1+self.n) ))\n",
    "\n",
    "    def expand(self):\n",
    "        for i in range(self.actions):\n",
    "            board_copy = copy.deepcopy(self.board)\n",
    "            if is_valid_location(board_copy,i):\n",
    "                row = get_next_open_row(board_copy,i)\n",
    "                drop_piece(board_copy,row,i,self.player) \n",
    "                won = winning_move(board_copy,self.player)\n",
    "                child = AZ_node(board_copy, i, self.player*-1, model=self.model,parent=self,done=won, cupt=self.cupt, gamma=self.gamma)\n",
    "            else:\n",
    "                child = AZ_node(board_copy, i, self.player*-1, model=self.model,parent=self, done = False, cupt=self.cupt, gamma=self.gamma, valid =False)\n",
    "            self.children.append(child)\n",
    "\n",
    "    def mcts_propagate(self, reward):\n",
    "        self.q_value = self.q_value*self.n + reward*self.player\n",
    "        self.n+=1\n",
    "        self.q_value = self.q_value / self.n\n",
    "        if self.parent is not None:\n",
    "            self.parent.mcts_propagate(reward)\n",
    "\n",
    "    def sampled_prob_vector(self):\n",
    "        return [x.n/self.n for x in self.children]\n",
    "\n",
    "class Alpha_player():\n",
    "    def __init__(self,model,simulations, actions=7,gamma=0.9, cupt =1):\n",
    "        self.simulations = simulations\n",
    "        self.actions = actions\n",
    "        self.gamma = gamma\n",
    "        self.cupt = cupt\n",
    "        self.model = model\n",
    "        \n",
    "    def make_choice(self,board,actions_,player):\n",
    "        \n",
    "        root = AZ_node(board, model = self.model, player=player,action=None, cupt=self.cupt)\n",
    "        for i in range(self.simulations):\n",
    "            node, reward = root.perform_mcts_search() #gets valid options frm board\n",
    "            node.mcts_propagate(reward)\n",
    "        col = np.argmax(root.sampled_prob_vector())\n",
    "        print([np.random.normal(loc=10,scale=10)*x.ucb_value() for x in root.children])\n",
    "        time.sleep(5)\n",
    "        return col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.normal(loc=1,scale=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Linear_Model()\n",
    "alfie = Alpha_player(simulations=120, model = model,cupt=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-  -  -  -  -  -  -\n",
      "0  0  0  0  0  0  0\n",
      "0  0  0  0  0  0  0\n",
      "0  0  0  0  0  0  0\n",
      "0  0  0  0  0  0  0\n",
      "0  0  0  0  0  0  0\n",
      "0  2  0  0  0  1  0\n",
      "-  -  -  -  -  -  -\n",
      "Move played: 1\n",
      "[tensor([0.1953], grad_fn=<AddBackward0>), tensor([0.1982], grad_fn=<AddBackward0>), tensor([0.1979], grad_fn=<AddBackward0>), tensor([0.1959], grad_fn=<AddBackward0>), tensor([0.1976], grad_fn=<AddBackward0>), tensor([0.1961], grad_fn=<AddBackward0>), tensor([0.1968], grad_fn=<AddBackward0>)]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-034406425ea9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mboard\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_board\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplay_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malfie\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'human'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-6683ba37c693>\u001b[0m in \u001b[0;36mplay_game\u001b[0;34m(init_board, agent_1, agent_2, printy, starting_player)\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent_2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplayer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprinty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-6683ba37c693>\u001b[0m in \u001b[0;36mturn\u001b[0;34m(player, board, agent, valid)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mvalid_acts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_valid_locations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mROW_COUNT\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mROW_COUNT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mCOLUMN_COUNT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCOLUMN_COUNT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_choice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid_acts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mplayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_valid_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-d0366ce921d6>\u001b[0m in \u001b[0;36mmake_choice\u001b[0;34m(self, board, actions_, player)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampled_prob_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mucb_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "board=create_board()\n",
    "play_game(board, agent_1 = alfie, agent_2 = 'human')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## def pit_two_agents(agent_1, agent_2, matches):\n",
    "    winners = []\n",
    "    for i in range(matches//2):\n",
    "        board = create_board()\n",
    "        w = play_game(board, agent_1, agent_2, printy= False)\n",
    "        winners.append(w)\n",
    "        board=create_board()\n",
    "        w = play_game(board, agent_1, agent_2, printy= False, starting_player=-1)\n",
    "        winners.append(w)\n",
    "        \n",
    "    return winners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "magic_monty1 = monte_carlo_player(50,cupt=1)\n",
    "pit_result = pit_two_agents(alfie, magic_monty1, 200)\n",
    "pit_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, training_epochs, games_per_epoch, mcts_search, batch_size, crit1, crit2, mini_epochs, cupt):\n",
    "    candidate_mod = Linear_Model()\n",
    "    candidate_mod.load_state_dict(model.state_dict())\n",
    "    for i in range(training_epochs):\n",
    "        m = 0\n",
    "        episodes = {}\n",
    " \n",
    "        for game in range(games_per_epoch):\n",
    "            states, prob_vecs, rewards = [], [] ,[]\n",
    "            states, prob_vecs, rewards= run_episode(mcts_search, candidate_mod, cupt, states, prob_vecs, rewards)\n",
    "            m+=len(rewards)\n",
    "            episodes[game] = {\n",
    "                              'states':states,\n",
    "                              'prob_vecs': prob_vecs,\n",
    "                              'rewards':rewards,\n",
    "                                'm': m\n",
    "                             }\n",
    "            \n",
    "            \n",
    "        x, y_probs, y_rewards = tensorfy_data(episodes,m)    \n",
    "        dLoader = dataloader_func(x, y_probs, y_rewards, batch_size)\n",
    "        candidate_mod = train_system(dLoader, candidate_mod, optimizer, crit1, crit2, mini_epochs)\n",
    "    return 1, 2, 3\n",
    "    candidate_agent = Alpha_player(simulations = mcts_searches, model = candidate_model, cupt=1)\n",
    "    old_agent = Alpha_player(simulations = mcts_searches, model = model, cupt=1)\n",
    "    score_v_monty = pit_two_agents(candidate_agent, magic_monty1, 4)\n",
    "    print(f'Score v Monty: {np.mean(score_v_monty)}')\n",
    "    score_v_old_agent = pit_two_agents(candidate_agent, magic_monty1, 5)\n",
    "    print(f'Score v old agent: {np.mean(score_v_old_agent)}')\n",
    "    return candidate_mod, score_v_monty, score_v_old_agent\n",
    "\n",
    "def run_episode(mcts_searches,candidate_mod,cupt, states, prob_vecs, rewards, gamma=0.9):\n",
    "    \n",
    "    board = create_board()\n",
    "    current_player = 1\n",
    "    is_done = False\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        root = AZ_node(board, action= None, player=current_player, model=candidate_mod, cupt=cupt, gamma=gamma) #node with starting board\n",
    "        \n",
    "        for i in range(mcts_searches):\n",
    "            node, reward = root.perform_mcts_search() #gets valid options frm board\n",
    "            node.mcts_propagate(reward)\n",
    "            \n",
    "        #log \n",
    "        states.append(root.board.flatten() * root.player) #if current_player = -1, store the state as *= -1 so it's in first person mode.\n",
    "        prob_vecs.append(root.sampled_prob_vector())\n",
    "        rewards.append(current_player) #we can use this to multiply by the reward later\n",
    "        \n",
    "        #makemove\n",
    "        col = np.argmax(root.sampled_prob_vector())\n",
    "        row = get_next_open_row(board,col)\n",
    "        drop_piece(board,row,col,current_player)    \n",
    "        \n",
    "        #check win and add rewards\n",
    "        if winning_move(board,current_player):\n",
    "            reward = current_player\n",
    "            rewards = [x*reward for x in rewards] \n",
    "            break\n",
    "        if board.all() != 0:\n",
    "            reward = 0\n",
    "            rewards = [x*0 for x in rewards]\n",
    "            break\n",
    "        current_player*=-1\n",
    "    return states, prob_vecs, rewards\n",
    "\n",
    "def tensorfy_data(episodes,m):\n",
    "    x = torch.zeros((m,42))\n",
    "    y_P = torch.zeros((m,7))\n",
    "    y_R = torch.zeros((m,1))\n",
    "    progress = 0 \n",
    "    for value in episodes.values():\n",
    "        x[progress:value['m']] = torch.from_numpy(np.array(value['states']))\n",
    "        y_P[progress:value['m']] = torch.from_numpy(np.array(value['prob_vecs']))\n",
    "        y_R[progress:value['m']] = torch.from_numpy(np.array(value['rewards']).reshape(-1,1))\n",
    "        progress = value['m']\n",
    "    return x, y_P, y_R\n",
    "\n",
    "def dataloader_func(x, y_probs, y_rewards, batch_size):\n",
    "    dataset = Dataset(x, y_probs, y_rewards)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,shuffle=True)\n",
    "    return dataloader\n",
    "    \n",
    "def train_system(dataloader, model, optimizer, crit1, crit2, epochs):\n",
    "    model.train()\n",
    "    for x, y_p, y_r in dataloader:\n",
    "        x, y_p, y_r = x.float(), y_p.float(), y_r.float()\n",
    "        probas, reward = model(x)\n",
    "        loss1 = crit1(reward, y_r)\n",
    "        loss2 = crit2(probas, y_p)\n",
    "        loss = loss1 - loss2\n",
    "        print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step(); optimizer.zero_grad()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "####NEED TO REMOVE GAMMA\n",
    "model = Linear_Model()\n",
    "optimizer = torch.optim.RMSprop(model.parameters(),lr=0.001)\n",
    "crit_1 = torch.nn.MSELoss()\n",
    "def prob_loss(probas, y_p):\n",
    "    return torch.mean(torch.sum(y_p * torch.log(probas),dim=1))\n",
    "crit_2 = prob_loss\n",
    "mcts_searches = 300\n",
    "training_epochs = 2\n",
    "games_per_epoch = 2\n",
    "batch_size = 32\n",
    "mini_epochs = 2\n",
    "cupt = 2\n",
    "cand_mod, score_v_monty, score_v_old_agent = training_loop(model, training_epochs, games_per_epoch, mcts_searches, batch_size, crit_1, crit_2, mini_epochs, cupt)\n",
    "# score_v_monty, score_v_old_agent = training_loop(model, training_epochs, games_per_epoch, mcts_searches, batch_size, crit_1, crit_2, mini_epochs, cupt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "Kernel is dead",
     "output_type": "error",
     "traceback": [
      "Error: Kernel is dead",
      "at g._sendKernelShellControl (/Users/jacoblourie/.vscode/extensions/ms-toolsai.jupyter-2021.5.745244803/out/client/extension.js:47:847444)",
      "at g.sendShellMessage (/Users/jacoblourie/.vscode/extensions/ms-toolsai.jupyter-2021.5.745244803/out/client/extension.js:47:847213)",
      "at g.requestExecute (/Users/jacoblourie/.vscode/extensions/ms-toolsai.jupyter-2021.5.745244803/out/client/extension.js:47:849755)",
      "at d.requestExecute (/Users/jacoblourie/.vscode/extensions/ms-toolsai.jupyter-2021.5.745244803/out/client/extension.js:32:335823)",
      "at w.requestExecute (/Users/jacoblourie/.vscode/extensions/ms-toolsai.jupyter-2021.5.745244803/out/client/extension.js:24:130165)",
      "at w.executeCodeCell (/Users/jacoblourie/.vscode/extensions/ms-toolsai.jupyter-2021.5.745244803/out/client/extension.js:49:635259)",
      "at w.execute (/Users/jacoblourie/.vscode/extensions/ms-toolsai.jupyter-2021.5.745244803/out/client/extension.js:49:634882)",
      "at w.start (/Users/jacoblourie/.vscode/extensions/ms-toolsai.jupyter-2021.5.745244803/out/client/extension.js:49:629791)",
      "at runMicrotasks (<anonymous>)",
      "at processTicksAndRejections (internal/process/task_queues.js:97:5)",
      "at async t.CellExecutionQueue.executeQueuedCells (/Users/jacoblourie/.vscode/extensions/ms-toolsai.jupyter-2021.5.745244803/out/client/extension.js:49:644977)",
      "at async t.CellExecutionQueue.start (/Users/jacoblourie/.vscode/extensions/ms-toolsai.jupyter-2021.5.745244803/out/client/extension.js:49:644517)"
     ]
    }
   ],
   "source": [
    "score_v_monty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit ('base': conda)",
   "name": "pythonjvsc74a57bd03140f74d8e1d28421ccd34109d4bdb636d1200d25e7dadfa8785402db4096f74"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "metadata": {
   "interpreter": {
    "hash": "3140f74d8e1d28421ccd34109d4bdb636d1200d25e7dadfa8785402db4096f74"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}